{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import PIL \n",
    "from PIL import Image\n",
    "from IPython.display import display\n",
    "import matplotlib.pyplot as plt\n",
    "import os \n",
    "import pathlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "# ƒê∆∞·ªùng d·∫´n ƒë·∫øn th∆∞ m·ª•c ch·ª©a ·∫£nh\n",
    "image_dir = \"data\\\\test_img\"\n",
    "\n",
    "# ƒê∆∞·ªùng d·∫´n ƒë·∫øn th∆∞ m·ª•c ch·ª©a t·ªáp nh√£n\n",
    "labels_dir = \"data\\\\test_label\"\n",
    "\n",
    "# T·∫°o th∆∞ m·ª•c m·ªõi ƒë·ªÉ l∆∞u k·∫øt qu·∫£\n",
    "output_dir = \"data\\\\output_val\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Duy·ªát qua c√°c t·ªáp trong th∆∞ m·ª•c ·∫£nh\n",
    "for filename in os.listdir(image_dir):\n",
    "    # L·∫•y t√™n t·ªáp ·∫£nh\n",
    "    image_filename = os.path.basename(filename)\n",
    "\n",
    "    # T√¨m t·ªáp nh√£n t∆∞∆°ng ·ª©ng\n",
    "    label_filename = os.path.join(labels_dir, os.path.splitext(image_filename)[0] + \".txt\")\n",
    "\n",
    "    # Sao ch√©p t·ªáp ·∫£nh v√† t·ªáp nh√£n v√†o th∆∞ m·ª•c ƒë·∫ßu ra\n",
    "    shutil.copy(os.path.join(image_dir, filename), os.path.join(output_dir, filename))\n",
    "    shutil.copy(label_filename, os.path.join(output_dir, os.path.splitext(image_filename)[0] + \".txt\"))\n",
    "\n",
    "print(\"ƒê√£ sao ch√©p th√†nh c√¥ng c√°c t·ªáp ·∫£nh v√† t·ªáp nh√£n v√†o th∆∞ m·ª•c ƒë·∫ßu ra.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics YOLOv8.1.24 üöÄ Python-3.10.12 torch-2.2.1+cu121 CUDA:0 (NVIDIA GeForce RTX 3060, 12037MiB)\n",
      "\u001b[34m\u001b[1mengine/trainer: \u001b[0mtask=detect, mode=train, model=yolov8n.pt, data=/home/ai/Desktop/yolov8/data/ttt.yaml, epochs=10, time=None, patience=100, batch=16, imgsz=640, save=True, save_period=-1, cache=False, device=None, workers=8, project=None, name=train2, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=runs/detect/train2\n",
      "Overriding model.yaml nc=80 with nc=1\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
      "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
      "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
      "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
      "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
      "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
      "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
      "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
      "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n",
      " 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n",
      " 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
      " 22        [15, 18, 21]  1    751507  ultralytics.nn.modules.head.Detect           [1, [64, 128, 256]]           \n",
      "Model summary: 225 layers, 3011043 parameters, 3011027 gradients, 8.2 GFLOPs\n",
      "\n",
      "Transferred 319/355 items from pretrained weights\n",
      "Freezing layer 'model.22.dfl.conv.weight'\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks with YOLOv8n...\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ‚úÖ\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /home/ai/Desktop/yolov8/data/output... 5924 images, 0 backgrounds, 0 corrupt: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5924/5924 [00:07<00:00, 810.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: /home/ai/Desktop/yolov8/data/output.cache\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /home/ai/Desktop/yolov8/data/output_val... 230 images, 0 backgrounds, 0 corrupt: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 230/230 [00:00<00:00, 1006.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /home/ai/Desktop/yolov8/data/output_val.cache\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plotting labels to runs/detect/train2/labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.002, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n",
      "Image sizes 640 train, 640 val\n",
      "Using 8 dataloader workers\n",
      "Logging results to \u001b[1mruns/detect/train2\u001b[0m\n",
      "Starting training for 10 epochs...\n",
      "Closing dataloader mosaic\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       1/10      2.09G      1.208      2.412      1.451         18        640:  39%|‚ñà‚ñà‚ñà‚ñä      | 143/371 [00:16<00:24,  9.35it/s]Corrupt JPEG data: 240 extraneous bytes before marker 0xd9\n",
      "       1/10       2.1G      1.206      2.025       1.43         18        640:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 272/371 [00:30<00:10,  9.71it/s]Corrupt JPEG data: premature end of data segment\n",
      "       1/10      2.13G       1.22      1.835      1.437          4        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 371/371 [00:41<00:00,  9.03it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8/8 [00:00<00:00,  8.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        230        244      0.854      0.744      0.851      0.501\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       2/10      2.21G       1.31      1.087      1.477         16        640:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 233/371 [00:24<00:14,  9.61it/s]Corrupt JPEG data: premature end of data segment\n",
      "Corrupt JPEG data: 240 extraneous bytes before marker 0xd9\n",
      "       2/10      2.25G      1.316      1.035       1.48          4        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 371/371 [00:38<00:00,  9.65it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8/8 [00:00<00:00, 12.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        230        244      0.841      0.705      0.802       0.47\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       3/10      2.21G      1.303     0.8882      1.453         17        640:  31%|‚ñà‚ñà‚ñà‚ñè      | 116/371 [00:11<00:25,  9.99it/s]Corrupt JPEG data: 240 extraneous bytes before marker 0xd9\n",
      "       3/10      2.21G      1.302     0.8791      1.452         16        640:  40%|‚ñà‚ñà‚ñà‚ñà      | 149/371 [00:15<00:22,  9.96it/s]Corrupt JPEG data: premature end of data segment\n",
      "       3/10      2.24G      1.296     0.8682      1.457          4        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 371/371 [00:37<00:00,  9.77it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8/8 [00:00<00:00, 12.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        230        244      0.835      0.807      0.858      0.469\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       4/10      2.21G       1.25     0.7869      1.416         16        640:  29%|‚ñà‚ñà‚ñâ       | 107/371 [00:11<00:27,  9.73it/s]Corrupt JPEG data: premature end of data segment\n",
      "       4/10      2.21G      1.256     0.7848      1.422         17        640:  34%|‚ñà‚ñà‚ñà‚ñé      | 125/371 [00:12<00:25,  9.84it/s]Corrupt JPEG data: 240 extraneous bytes before marker 0xd9\n",
      "       4/10      2.25G      1.239     0.7717      1.413          4        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 371/371 [00:37<00:00,  9.78it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8/8 [00:00<00:00, 12.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        230        244      0.947      0.877      0.963      0.603\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       5/10      2.21G      1.177      0.711      1.369         16        640:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 264/371 [00:26<00:10,  9.94it/s]Corrupt JPEG data: 240 extraneous bytes before marker 0xd9\n",
      "       5/10      2.21G      1.178      0.712       1.37         18        640:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 346/371 [00:35<00:02,  9.45it/s]Corrupt JPEG data: premature end of data segment\n",
      "       5/10      2.25G      1.176     0.7091      1.367          4        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 371/371 [00:37<00:00,  9.79it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8/8 [00:00<00:00, 13.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        230        244       0.87      0.898      0.943      0.602\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       6/10      2.21G       1.13     0.6583      1.329         16        640:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 217/371 [00:22<00:15,  9.68it/s]Corrupt JPEG data: 240 extraneous bytes before marker 0xd9\n",
      "       6/10      2.21G      1.118     0.6504      1.323         17        640:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 316/371 [00:32<00:05,  9.75it/s]Corrupt JPEG data: premature end of data segment\n",
      "       6/10      2.25G       1.12     0.6483      1.327          4        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 371/371 [00:38<00:00,  9.71it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8/8 [00:00<00:00, 13.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        230        244      0.943      0.875      0.967       0.63\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       7/10      2.21G     0.9568     0.6009      1.236         18        640:   1%|          | 2/371 [00:00<00:38,  9.49it/s]Corrupt JPEG data: premature end of data segment\n",
      "       7/10      2.21G      1.055     0.5759      1.273         17        640:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 154/371 [00:15<00:22,  9.62it/s]Corrupt JPEG data: 240 extraneous bytes before marker 0xd9\n",
      "       7/10      2.24G      1.053     0.5721      1.274          4        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 371/371 [00:37<00:00,  9.79it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8/8 [00:00<00:00, 12.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        230        244       0.97      0.929      0.982      0.664\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       8/10      2.21G      1.016     0.5312      1.257         16        640:  40%|‚ñà‚ñà‚ñà‚ñâ      | 147/371 [00:15<00:23,  9.59it/s]Corrupt JPEG data: 240 extraneous bytes before marker 0xd9\n",
      "       8/10      2.21G      1.018     0.5317      1.256         16        640:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 201/371 [00:20<00:17,  9.77it/s]Corrupt JPEG data: premature end of data segment\n",
      "       8/10      2.25G      1.017     0.5306      1.248          4        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 371/371 [00:38<00:00,  9.73it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8/8 [00:00<00:00, 12.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        230        244       0.93      0.951      0.979      0.667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       9/10      2.21G      1.004     0.5169      1.228         17        640:  20%|‚ñà‚ñâ        | 74/371 [00:07<00:30,  9.79it/s]Corrupt JPEG data: 240 extraneous bytes before marker 0xd9\n",
      "       9/10      2.21G     0.9691     0.4903      1.213         16        640:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 348/371 [00:35<00:02,  9.91it/s]Corrupt JPEG data: premature end of data segment\n",
      "       9/10      2.25G     0.9704     0.4905      1.213          5        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 371/371 [00:38<00:00,  9.72it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8/8 [00:00<00:00, 13.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        230        244      0.976      0.967      0.992      0.693\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      10/10      2.21G     0.9509     0.4618      1.199         16        640:  37%|‚ñà‚ñà‚ñà‚ñã      | 139/371 [00:14<00:23,  9.90it/s]Corrupt JPEG data: 240 extraneous bytes before marker 0xd9\n",
      "      10/10      2.21G     0.9338     0.4557      1.189         17        640:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 319/371 [00:32<00:05,  9.95it/s]Corrupt JPEG data: premature end of data segment\n",
      "      10/10      2.25G     0.9308     0.4546      1.185          5        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 371/371 [00:37<00:00,  9.82it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8/8 [00:00<00:00, 13.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        230        244      0.968      0.978      0.992      0.714\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "10 epochs completed in 0.109 hours.\n",
      "Optimizer stripped from runs/detect/train2/weights/last.pt, 6.2MB\n",
      "Optimizer stripped from runs/detect/train2/weights/best.pt, 6.2MB\n",
      "\n",
      "Validating runs/detect/train2/weights/best.pt...\n",
      "Ultralytics YOLOv8.1.24 üöÄ Python-3.10.12 torch-2.2.1+cu121 CUDA:0 (NVIDIA GeForce RTX 3060, 12037MiB)\n",
      "Model summary (fused): 168 layers, 3005843 parameters, 0 gradients, 8.1 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8/8 [00:00<00:00,  8.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        230        244      0.968      0.977      0.992      0.715\n",
      "Speed: 0.2ms preprocess, 1.8ms inference, 0.0ms loss, 0.8ms postprocess per image\n",
      "Results saved to \u001b[1mruns/detect/train2\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "# Load a model\n",
    "model = YOLO('yolov8n.pt')  # load a pretrained model (recommended for training)\n",
    "\n",
    "# Train the model\n",
    "results = model.train(data='/home/ai/Desktop/yolov8/data/ttt.yaml', epochs=10, imgsz=640)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "import supervision as sv\n",
    "import numpy as np\n",
    "\n",
    "# Load a pretrained YOLOv8 model\n",
    "model = YOLO(\"runs/detect/train2/weights/best.pt\")\n",
    "\n",
    "# Load video info\n",
    "VIDEO_PATH = \"/home/ai/Desktop/yolov8/video (1080p).mp4\"\n",
    "video_info = sv.VideoInfo.from_video_path(VIDEO_PATH)\n",
    "\n",
    "# Create a callback to process each frame\n",
    "def process_frame(frame: np.ndarray, _) -> np.ndarray:\n",
    "    results = model(frame, imgsz=128)[0]\n",
    "    detections = sv.Detections.from_ultralytics(results)\n",
    "    box_annotator = sv.BoxAnnotator(thickness=4, text_thickness=4, text_scale=2)\n",
    "    labels = [f\"{model.names[class_id]} {confidence:.2f}\" for _, _, confidence, class_id, _ in detections]\n",
    "    frame = box_annotator.annotate(scene=frame, detections=detections, labels=labels)\n",
    "    return frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 128x96 (no detections), 3.2ms\n",
      "Speed: 0.3ms preprocess, 3.2ms inference, 0.3ms postprocess per image at shape (1, 3, 128, 96)\n",
      "\n",
      "0: 128x96 (no detections), 3.6ms\n",
      "Speed: 0.6ms preprocess, 3.6ms inference, 0.3ms postprocess per image at shape (1, 3, 128, 96)\n",
      "\n",
      "0: 128x96 (no detections), 4.3ms\n",
      "Speed: 0.8ms preprocess, 4.3ms inference, 0.3ms postprocess per image at shape (1, 3, 128, 96)\n",
      "\n",
      "0: 128x96 (no detections), 3.7ms\n",
      "Speed: 0.7ms preprocess, 3.7ms inference, 0.3ms postprocess per image at shape (1, 3, 128, 96)\n",
      "\n",
      "0: 128x96 (no detections), 3.4ms\n",
      "Speed: 0.5ms preprocess, 3.4ms inference, 0.3ms postprocess per image at shape (1, 3, 128, 96)\n",
      "\n",
      "0: 128x96 (no detections), 3.4ms\n",
      "Speed: 0.5ms preprocess, 3.4ms inference, 0.6ms postprocess per image at shape (1, 3, 128, 96)\n",
      "\n",
      "0: 128x96 (no detections), 3.9ms\n",
      "Speed: 0.8ms preprocess, 3.9ms inference, 0.3ms postprocess per image at shape (1, 3, 128, 96)\n",
      "\n",
      "0: 128x96 (no detections), 3.6ms\n",
      "Speed: 0.6ms preprocess, 3.6ms inference, 0.3ms postprocess per image at shape (1, 3, 128, 96)\n",
      "\n",
      "0: 128x96 (no detections), 3.4ms\n",
      "Speed: 0.5ms preprocess, 3.4ms inference, 0.3ms postprocess per image at shape (1, 3, 128, 96)\n",
      "\n",
      "0: 128x96 (no detections), 4.1ms\n",
      "Speed: 0.6ms preprocess, 4.1ms inference, 0.3ms postprocess per image at shape (1, 3, 128, 96)\n",
      "\n",
      "0: 128x96 (no detections), 3.5ms\n",
      "Speed: 0.6ms preprocess, 3.5ms inference, 0.8ms postprocess per image at shape (1, 3, 128, 96)\n",
      "\n",
      "0: 128x96 (no detections), 12.7ms\n",
      "Speed: 1.1ms preprocess, 12.7ms inference, 1.2ms postprocess per image at shape (1, 3, 128, 96)\n",
      "\n",
      "0: 128x96 (no detections), 4.2ms\n",
      "Speed: 0.6ms preprocess, 4.2ms inference, 0.5ms postprocess per image at shape (1, 3, 128, 96)\n",
      "\n",
      "0: 128x96 (no detections), 3.6ms\n",
      "Speed: 0.6ms preprocess, 3.6ms inference, 0.3ms postprocess per image at shape (1, 3, 128, 96)\n",
      "\n",
      "0: 128x96 (no detections), 4.5ms\n",
      "Speed: 0.7ms preprocess, 4.5ms inference, 0.3ms postprocess per image at shape (1, 3, 128, 96)\n",
      "\n",
      "0: 128x96 (no detections), 3.3ms\n",
      "Speed: 0.6ms preprocess, 3.3ms inference, 0.3ms postprocess per image at shape (1, 3, 128, 96)\n",
      "\n",
      "0: 128x96 (no detections), 3.6ms\n",
      "Speed: 0.7ms preprocess, 3.6ms inference, 0.3ms postprocess per image at shape (1, 3, 128, 96)\n",
      "\n",
      "0: 128x96 (no detections), 4.3ms\n",
      "Speed: 0.6ms preprocess, 4.3ms inference, 0.3ms postprocess per image at shape (1, 3, 128, 96)\n",
      "\n",
      "0: 128x96 (no detections), 3.4ms\n",
      "Speed: 0.5ms preprocess, 3.4ms inference, 0.3ms postprocess per image at shape (1, 3, 128, 96)\n",
      "\n",
      "0: 128x96 (no detections), 3.4ms\n",
      "Speed: 0.4ms preprocess, 3.4ms inference, 0.3ms postprocess per image at shape (1, 3, 128, 96)\n",
      "\n",
      "0: 128x96 (no detections), 3.4ms\n",
      "Speed: 0.4ms preprocess, 3.4ms inference, 0.3ms postprocess per image at shape (1, 3, 128, 96)\n",
      "\n",
      "0: 128x96 (no detections), 3.6ms\n",
      "Speed: 0.6ms preprocess, 3.6ms inference, 0.3ms postprocess per image at shape (1, 3, 128, 96)\n",
      "\n",
      "0: 128x96 (no detections), 3.7ms\n",
      "Speed: 0.9ms preprocess, 3.7ms inference, 0.3ms postprocess per image at shape (1, 3, 128, 96)\n",
      "\n",
      "0: 128x96 (no detections), 3.4ms\n",
      "Speed: 0.8ms preprocess, 3.4ms inference, 0.3ms postprocess per image at shape (1, 3, 128, 96)\n",
      "\n",
      "0: 128x96 (no detections), 3.5ms\n",
      "Speed: 0.5ms preprocess, 3.5ms inference, 0.3ms postprocess per image at shape (1, 3, 128, 96)\n",
      "\n",
      "0: 128x96 (no detections), 3.5ms\n",
      "Speed: 0.5ms preprocess, 3.5ms inference, 0.3ms postprocess per image at shape (1, 3, 128, 96)\n",
      "\n",
      "0: 128x96 (no detections), 3.4ms\n",
      "Speed: 0.5ms preprocess, 3.4ms inference, 0.3ms postprocess per image at shape (1, 3, 128, 96)\n",
      "\n",
      "0: 128x96 (no detections), 3.4ms\n",
      "Speed: 0.5ms preprocess, 3.4ms inference, 0.3ms postprocess per image at shape (1, 3, 128, 96)\n",
      "\n",
      "0: 128x96 (no detections), 3.4ms\n",
      "Speed: 0.5ms preprocess, 3.4ms inference, 0.3ms postprocess per image at shape (1, 3, 128, 96)\n",
      "\n",
      "0: 128x96 (no detections), 3.4ms\n",
      "Speed: 0.5ms preprocess, 3.4ms inference, 0.3ms postprocess per image at shape (1, 3, 128, 96)\n",
      "\n",
      "0: 128x96 (no detections), 3.4ms\n",
      "Speed: 0.6ms preprocess, 3.4ms inference, 0.3ms postprocess per image at shape (1, 3, 128, 96)\n",
      "\n",
      "0: 128x96 (no detections), 4.4ms\n",
      "Speed: 0.4ms preprocess, 4.4ms inference, 0.4ms postprocess per image at shape (1, 3, 128, 96)\n",
      "\n",
      "0: 128x96 (no detections), 3.6ms\n",
      "Speed: 0.8ms preprocess, 3.6ms inference, 0.3ms postprocess per image at shape (1, 3, 128, 96)\n",
      "\n",
      "0: 128x96 (no detections), 3.6ms\n",
      "Speed: 0.6ms preprocess, 3.6ms inference, 0.3ms postprocess per image at shape (1, 3, 128, 96)\n",
      "\n",
      "0: 128x96 (no detections), 3.7ms\n",
      "Speed: 0.7ms preprocess, 3.7ms inference, 0.3ms postprocess per image at shape (1, 3, 128, 96)\n",
      "\n",
      "0: 128x96 (no detections), 3.7ms\n",
      "Speed: 0.6ms preprocess, 3.7ms inference, 0.3ms postprocess per image at shape (1, 3, 128, 96)\n",
      "\n",
      "0: 128x96 (no detections), 3.5ms\n",
      "Speed: 0.5ms preprocess, 3.5ms inference, 0.4ms postprocess per image at shape (1, 3, 128, 96)\n",
      "\n",
      "0: 128x96 (no detections), 4.1ms\n",
      "Speed: 0.6ms preprocess, 4.1ms inference, 0.4ms postprocess per image at shape (1, 3, 128, 96)\n",
      "\n",
      "0: 128x96 (no detections), 3.6ms\n",
      "Speed: 0.7ms preprocess, 3.6ms inference, 0.3ms postprocess per image at shape (1, 3, 128, 96)\n",
      "\n",
      "0: 128x96 (no detections), 3.6ms\n",
      "Speed: 0.6ms preprocess, 3.6ms inference, 0.3ms postprocess per image at shape (1, 3, 128, 96)\n",
      "\n",
      "0: 128x96 (no detections), 3.7ms\n",
      "Speed: 0.5ms preprocess, 3.7ms inference, 0.3ms postprocess per image at shape (1, 3, 128, 96)\n",
      "\n",
      "0: 128x96 (no detections), 3.4ms\n",
      "Speed: 0.8ms preprocess, 3.4ms inference, 0.3ms postprocess per image at shape (1, 3, 128, 96)\n",
      "\n",
      "0: 128x96 (no detections), 7.5ms\n",
      "Speed: 0.8ms preprocess, 7.5ms inference, 0.6ms postprocess per image at shape (1, 3, 128, 96)\n",
      "\n",
      "0: 128x96 (no detections), 3.1ms\n",
      "Speed: 0.6ms preprocess, 3.1ms inference, 0.3ms postprocess per image at shape (1, 3, 128, 96)\n",
      "\n",
      "0: 128x96 (no detections), 3.3ms\n",
      "Speed: 0.4ms preprocess, 3.3ms inference, 0.3ms postprocess per image at shape (1, 3, 128, 96)\n",
      "\n",
      "0: 128x96 (no detections), 3.3ms\n",
      "Speed: 0.5ms preprocess, 3.3ms inference, 0.3ms postprocess per image at shape (1, 3, 128, 96)\n"
     ]
    }
   ],
   "source": [
    "sv.process_video(source_path=VIDEO_PATH, target_path=\"result.mp4\", callback=process_frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "import supervision as sv\n",
    "import numpy as np\n",
    "\n",
    "# Load a pretrained YOLOv8 model\n",
    "model = YOLO(\"runs/detect/train2/weights/best.pt\")\n",
    "\n",
    "# Load video info\n",
    "VIDEO_PATH = \"/home/ai/Desktop/yolov8/video (1080p).mp4\"\n",
    "video_info = sv.VideoInfo.from_video_path(VIDEO_PATH)\n",
    "\n",
    "# Create a callback to process each frame\n",
    "def process_frame(frame: np.ndarray, _) -> np.ndarray:\n",
    "    results = model(frame, imgsz=128)[0]\n",
    "    detections = sv.Detections.from_ultralytics(results)\n",
    "    box_annotator = sv.BoxAnnotator(thickness=4, text_thickness=4, text_scale=2)\n",
    "    labels = [f\"{model.names[class_id]} {confidence:.2f}\" for _, _, confidence, class_id, _ in detections]\n",
    "    frame = box_annotator.annotate(scene=frame, detections=detections, labels=labels)\n",
    "    return frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 /home/ai/Desktop/yolov8/anh-che-cho-hai-huoc-cho-dien-thoai-14.jpg: 640x512 1 person, 3.5ms\n",
      "Speed: 1.3ms preprocess, 3.5ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 512)\n"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "# Load a model\n",
    "model = YOLO(\"runs/detect/train2/weights/best.pt\")  # Replace with your model path\n",
    "\n",
    "# Run batched inference on a list of images\n",
    "results = model(\"anh-che-cho-hai-huoc-cho-dien-thoai-14.jpg\")  # return a list of Results objects\n",
    "\n",
    "# Process results list\n",
    "for result in results:\n",
    "    boxes = result.boxes  # Boxes object for bounding box outputs\n",
    "    masks = result.masks  # Masks object for segmentation masks outputs\n",
    "    keypoints = result.keypoints  # Keypoints object for pose outputs\n",
    "    probs = result.probs  # Probs object for classification outputs\n",
    "    result.show()  # display to screen\n",
    "    result.save(filename='result.jpg')  # save to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = YOLO('runs/detect/train2/weights/best.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING ‚ö†Ô∏è Environment does not support cv2.imshow() or PIL Image.show()\n",
      "OpenCV(4.9.0) /io/opencv/modules/highgui/src/window.cpp:1272: error: (-2:Unspecified error) The function is not implemented. Rebuild the library with Windows, GTK+ 2.x or Cocoa support. If you are on Ubuntu or Debian, install libgtk2.0-dev and pkg-config, then re-run cmake or configure script in function 'cvShowImage'\n",
      "\n",
      "\n",
      "\n",
      "WARNING ‚ö†Ô∏è inference results will accumulate in RAM unless `stream=True` is passed, causing potential out-of-memory\n",
      "errors for large sources or long-running streams and videos. See https://docs.ultralytics.com/modes/predict/ for help.\n",
      "\n",
      "Example:\n",
      "    results = model(source=..., stream=True)  # generator of Results objects\n",
      "    for r in results:\n",
      "        boxes = r.boxes  # Boxes object for bbox outputs\n",
      "        masks = r.masks  # Masks object for segment masks outputs\n",
      "        probs = r.probs  # Class probabilities for classification outputs\n",
      "\n",
      "video 1/1 (1/46) /home/ai/Desktop/yolov8/video (1080p).mp4: 640x384 1 person, 5.5ms\n",
      "video 1/1 (2/46) /home/ai/Desktop/yolov8/video (1080p).mp4: 640x384 1 person, 3.8ms\n",
      "video 1/1 (3/46) /home/ai/Desktop/yolov8/video (1080p).mp4: 640x384 1 person, 4.1ms\n",
      "video 1/1 (4/46) /home/ai/Desktop/yolov8/video (1080p).mp4: 640x384 1 person, 4.4ms\n",
      "video 1/1 (5/46) /home/ai/Desktop/yolov8/video (1080p).mp4: 640x384 1 person, 3.5ms\n",
      "video 1/1 (6/46) /home/ai/Desktop/yolov8/video (1080p).mp4: 640x384 1 person, 4.5ms\n",
      "video 1/1 (7/46) /home/ai/Desktop/yolov8/video (1080p).mp4: 640x384 1 person, 3.6ms\n",
      "video 1/1 (8/46) /home/ai/Desktop/yolov8/video (1080p).mp4: 640x384 1 person, 4.0ms\n",
      "video 1/1 (9/46) /home/ai/Desktop/yolov8/video (1080p).mp4: 640x384 1 person, 5.1ms\n",
      "video 1/1 (10/46) /home/ai/Desktop/yolov8/video (1080p).mp4: 640x384 1 person, 3.3ms\n",
      "video 1/1 (11/46) /home/ai/Desktop/yolov8/video (1080p).mp4: 640x384 1 person, 3.4ms\n",
      "video 1/1 (12/46) /home/ai/Desktop/yolov8/video (1080p).mp4: 640x384 1 person, 3.6ms\n",
      "video 1/1 (13/46) /home/ai/Desktop/yolov8/video (1080p).mp4: 640x384 1 person, 3.2ms\n",
      "video 1/1 (14/46) /home/ai/Desktop/yolov8/video (1080p).mp4: 640x384 1 person, 3.7ms\n",
      "video 1/1 (15/46) /home/ai/Desktop/yolov8/video (1080p).mp4: 640x384 1 person, 3.5ms\n",
      "video 1/1 (16/46) /home/ai/Desktop/yolov8/video (1080p).mp4: 640x384 1 person, 3.4ms\n",
      "video 1/1 (17/46) /home/ai/Desktop/yolov8/video (1080p).mp4: 640x384 1 person, 3.5ms\n",
      "video 1/1 (18/46) /home/ai/Desktop/yolov8/video (1080p).mp4: 640x384 1 person, 3.5ms\n",
      "video 1/1 (19/46) /home/ai/Desktop/yolov8/video (1080p).mp4: 640x384 1 person, 3.5ms\n",
      "video 1/1 (20/46) /home/ai/Desktop/yolov8/video (1080p).mp4: 640x384 1 person, 3.2ms\n",
      "video 1/1 (21/46) /home/ai/Desktop/yolov8/video (1080p).mp4: 640x384 1 person, 4.5ms\n",
      "video 1/1 (22/46) /home/ai/Desktop/yolov8/video (1080p).mp4: 640x384 1 person, 3.4ms\n",
      "video 1/1 (23/46) /home/ai/Desktop/yolov8/video (1080p).mp4: 640x384 1 person, 4.7ms\n",
      "video 1/1 (24/46) /home/ai/Desktop/yolov8/video (1080p).mp4: 640x384 1 person, 4.7ms\n",
      "video 1/1 (25/46) /home/ai/Desktop/yolov8/video (1080p).mp4: 640x384 1 person, 3.5ms\n",
      "video 1/1 (26/46) /home/ai/Desktop/yolov8/video (1080p).mp4: 640x384 1 person, 3.6ms\n",
      "video 1/1 (27/46) /home/ai/Desktop/yolov8/video (1080p).mp4: 640x384 1 person, 3.4ms\n",
      "video 1/1 (28/46) /home/ai/Desktop/yolov8/video (1080p).mp4: 640x384 1 person, 3.3ms\n",
      "video 1/1 (29/46) /home/ai/Desktop/yolov8/video (1080p).mp4: 640x384 1 person, 3.6ms\n",
      "video 1/1 (30/46) /home/ai/Desktop/yolov8/video (1080p).mp4: 640x384 1 person, 3.4ms\n",
      "video 1/1 (31/46) /home/ai/Desktop/yolov8/video (1080p).mp4: 640x384 1 person, 3.7ms\n",
      "video 1/1 (32/46) /home/ai/Desktop/yolov8/video (1080p).mp4: 640x384 1 person, 3.4ms\n",
      "video 1/1 (33/46) /home/ai/Desktop/yolov8/video (1080p).mp4: 640x384 1 person, 3.5ms\n",
      "video 1/1 (34/46) /home/ai/Desktop/yolov8/video (1080p).mp4: 640x384 1 person, 3.4ms\n",
      "video 1/1 (35/46) /home/ai/Desktop/yolov8/video (1080p).mp4: 640x384 1 person, 3.4ms\n",
      "video 1/1 (36/46) /home/ai/Desktop/yolov8/video (1080p).mp4: 640x384 1 person, 3.9ms\n",
      "video 1/1 (37/46) /home/ai/Desktop/yolov8/video (1080p).mp4: 640x384 1 person, 3.4ms\n",
      "video 1/1 (38/46) /home/ai/Desktop/yolov8/video (1080p).mp4: 640x384 1 person, 3.5ms\n",
      "video 1/1 (39/46) /home/ai/Desktop/yolov8/video (1080p).mp4: 640x384 1 person, 3.4ms\n",
      "video 1/1 (40/46) /home/ai/Desktop/yolov8/video (1080p).mp4: 640x384 1 person, 3.4ms\n",
      "video 1/1 (41/46) /home/ai/Desktop/yolov8/video (1080p).mp4: 640x384 1 person, 4.0ms\n",
      "video 1/1 (42/46) /home/ai/Desktop/yolov8/video (1080p).mp4: 640x384 1 person, 3.6ms\n",
      "video 1/1 (43/46) /home/ai/Desktop/yolov8/video (1080p).mp4: 640x384 1 person, 4.0ms\n",
      "video 1/1 (44/46) /home/ai/Desktop/yolov8/video (1080p).mp4: 640x384 1 person, 3.4ms\n",
      "video 1/1 (45/46) /home/ai/Desktop/yolov8/video (1080p).mp4: 640x384 1 person, 4.0ms\n",
      "video 1/1 (46/46) /home/ai/Desktop/yolov8/video (1080p).mp4: 640x384 1 person, 3.7ms\n",
      "Speed: 1.3ms preprocess, 3.7ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 384)\n",
      "Results saved to \u001b[1mruns/detect/predict2\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Load the YOLOv8 model\n",
    "model = YOLO('runs/detect/train2/weights/best.pt')\n",
    "\n",
    "# Open the video file\n",
    "video_path = \"/home/ai/Desktop/yolov8/video (1080p).mp4\"\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "results = model.predict(source=video_path, save = True, show = True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
